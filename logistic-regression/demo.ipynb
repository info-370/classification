{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression\n",
    "\n",
    "In today's exercise, we'll introduce **logistic regression** as a mechanism for performing _classification tasks_. This exercise walks through a conceptual example discussd in chapter 4 of [this book](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf), and the data was extracted from the `ISLR` R package (specifically, the `Default` dataset). The question of interest for this exercise is, \n",
    "\n",
    "> Can we predict which individuals will default on their credit card payments?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up (0 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up\n",
    "import numpy as np\n",
    "from __future__ import division # division\n",
    "import pandas as pd\n",
    "import seaborn as sns # for visualiation\n",
    "from scipy.stats import ttest_ind # t-tests\n",
    "import statsmodels.formula.api as smf # linear modeling\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib\n",
    "from sklearn import metrics\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data, replace strings as numeric\n",
    "df = pd.read_csv('./data/payment-default.csv')\n",
    "df.default = df.default.replace(['Yes', 'No'], [1, 0]).astype(int)\n",
    "df.student = df.student.replace(['Yes', 'No'], [1, 0]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (15 minutes)\n",
    "\n",
    "In this first section, you'll explore your dataset to get a better handle on the distributions. The data for this exercise has **10,000** observations (people), who did, or did not, default on their credit card payment. The data has the following information for each person:\n",
    "\n",
    "- `default`: This is our **outcome** of interest, and is **binary** (yes/no)\n",
    "- `student`: This variable indicates if each person is a students (yes/no)\n",
    "- `balance`: Current balance on the credit card (**continuous**)\n",
    "- `income`: Annual income of the individual (**continuous**)\n",
    "\n",
    "In this section, you'll write the code necessary to answer the following questions:\n",
    "\n",
    "- What is the **default rate** in the dataset (# of defaults / total)\n",
    "- What is the distribution of credit card **balances** (for those who default, and those who do not)?\n",
    "- What is the relationship between `balance` and `income` (show a scatter-plot, different colors for default/no default)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No axis named defaults for object type <class 'pandas.core.frame.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f575e41bceb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# What is the **default rate** in the dataset (# of defaults / total)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'defaults'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, axis, level, numeric_only)\u001b[0m\n\u001b[1;32m   4693\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mSeries\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mor\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4694\u001b[0m         \"\"\"\n\u001b[0;32m-> 4695\u001b[0;31m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4696\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4697\u001b[0m             return self._count_level(level, axis=axis,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_axis_number\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         raise ValueError('No axis named {0} for object type {1}'\n\u001b[0;32m--> 318\u001b[0;31m                          .format(axis, type(self)))\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No axis named defaults for object type <class 'pandas.core.frame.DataFrame'>"
     ]
    }
   ],
   "source": [
    "# What is the **default rate** in the dataset (# of defaults / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the distribution of balances (for those who default, and those who don't)?\n",
    "\n",
    "# You may want to create subsets of the data for those who defaulted, and those who did not\n",
    "\n",
    "# Draw histograms of the distribution (perhaps overlapping histogram on the same chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the relationship between balalance and income (show a scatter-plot, different colors for default/no default)?\n",
    "# Hint: http://stackoverflow.com/questions/21654635/scatter-plots-in-pandas-pyplot-how-to-plot-by-category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List pertinent observations from the above analysis:\n",
    "\n",
    "> Your insights here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Linear Approach (15 minutes)\n",
    "\n",
    "Before introducing a new estimation approach, we'll observe the limitations of using a linear estimation of a binary outcome. In this section, you'll do the following:\n",
    "\n",
    "- Fit a **linear model** of the dependent variable (`default`) on _one_ indepdendent variable (`balance`)\n",
    "- Generate predictions using your model (this can be interpreted as _probability of default_)\n",
    "- Visualize the results (comparing `balance` to the predicted `default` rate)\n",
    "- Interpret the **coefficients** and assess the **model fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>default</td>     <th>  R-squared:         </th> <td>   0.123</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.122</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1397.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 13 Feb 2017</td> <th>  Prob (F-statistic):</th> <td>2.77e-286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:17:20</td>     <th>  Log-Likelihood:    </th> <td>  3644.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>  -7286.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  9998</td>      <th>  BIC:               </th> <td>  -7271.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.0752</td> <td>    0.003</td> <td>  -22.416</td> <td> 0.000</td> <td>   -0.082    -0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0001</td> <td> 3.47e-06</td> <td>   37.374</td> <td> 0.000</td> <td>    0.000     0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>8547.967</td> <th>  Durbin-Watson:     </th>  <td>   2.023</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>180810.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 4.243</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>22.025</td>  <th>  Cond. No.          </th>  <td>1.93e+03</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                default   R-squared:                       0.123\n",
       "Model:                            OLS   Adj. R-squared:                  0.122\n",
       "Method:                 Least Squares   F-statistic:                     1397.\n",
       "Date:                Mon, 13 Feb 2017   Prob (F-statistic):          2.77e-286\n",
       "Time:                        14:17:20   Log-Likelihood:                 3644.8\n",
       "No. Observations:               10000   AIC:                            -7286.\n",
       "Df Residuals:                    9998   BIC:                            -7271.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.0752      0.003    -22.416      0.000        -0.082    -0.069\n",
       "balance        0.0001   3.47e-06     37.374      0.000         0.000     0.000\n",
       "==============================================================================\n",
       "Omnibus:                     8547.967   Durbin-Watson:                   2.023\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           180810.806\n",
       "Skew:                           4.243   Prob(JB):                         0.00\n",
       "Kurtosis:                      22.025   Cond. No.                     1.93e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.93e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a linear model of the dependent variable (default) on balance\n",
    "lm = smf.ols(formula='default ~ balance', data=df).fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions using your model (this can be interpreted as probability of default)\n",
    "df['linear_preds'] = lm.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAFoCAYAAADkRdnBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nti16AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27b02a2db38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the results: scatterplot of balance v.s. predcited default rate. Bonus: add actual default rate.\n",
    "plt.scatter(df.linear_preds, df.predictions)\n",
    "plt.plot(df.default, df.predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpret the **coefficients** and assess the **model fit**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your interpretation of the coefficients and your assessment of the model fit?\n",
    "\n",
    "> Your insights here\n",
    "\n",
    "negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Logistic Function (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, you'll simply familiarize yourself with the **Logistic Function**. In order to perform **classification**, we want to explicitly **model the probability** of falling into each category. Given the limitations of linear regression, we want to find a functional form with the following property:\n",
    "\n",
    "> _All values output by the function should fall **between 0 and 1**._\n",
    "\n",
    "Because we are modeling **conditional probability**, we will annotate our formula using the following shorthand:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(X)= Pr\\left(Y=1|X\\right) \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "p(balance)=Pr(default=1|balance)\n",
    "\\end{equation*}\n",
    "\n",
    "While there are many functions that meet this requirement, a commonly used formula (and the one that we will use for logistic regression) is the **logistic function**:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "p(X) = \\frac { { e }^{ { { B }_{ 0 } } + { B }_{ 1 }X } }{ 1+{ e }^{ { { B }_{ 0 } } + { B }_{ 1 }X}} \n",
    "\\end{equation*}\n",
    "\n",
    "Note, with some manipulaiton, we can rearrange the above equation into the following equality:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac { p(X) }{ 1 - p(X) } = { e }^{ { { B }_{ 0 } }+{ B }_{ 1 }X } \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you may have noticed, the left hand side of the equation is the **odds** of observing our dependent variable given our independent variable(s). This is because the odds is the ratio of the probability of an event, $p(X)$, relative to the probability of not observing the event, $1 - p(X)$.\n",
    "\n",
    "By simply taking the log of both sides of the above equation, we arrive at the following formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\left( { \\frac { p(X) }{ 1-p(X) } }\\right) ={ { B }_{ 0 } }+{ B }_{ 1 }X\n",
    "\\end{equation*}\n",
    "\n",
    "On the left-hand side of the equation, we are left with the _log odds_, or _logit_, that has a **linear relationship** with the predictor variables. The betas are estimated using **maximum likelihood methods**, which are beyond the scope of this course. However, the intuition of this approach is that you are \"finding the set of parameters for which the probability of the observed data is greatest\" ([source](http://czep.net/stat/mlelr.pdf)). \n",
    "\n",
    "Logistic models will generate a set of probabilities of an observation being a 0 or a 1. Given the functional form, the default threshold for classification is .5 (i.e., if a probability is less than .5, the model predicts it is _not_ a positive case). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Logistic Regression (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue to use the `statsmodels.formula.api` to write R style formulas in our analytical approaches. Logistic Regression is considered a **Generalized Linear Model**, which is described well on [Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model):\n",
    "\n",
    "> The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models **other than a normal distribution**.\n",
    "\n",
    "This class of models extends the linear model in a variety of ways, while providing an API that resembles the linear approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you should: \n",
    "\n",
    "- Use the `smf.glm` function to fit `default` to `balance` (hint: see this [example](http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/glm_formula.html))\n",
    "- Generate a set of predicted probabilities using your model\n",
    "- Visualize the predicted probabilities across balances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>default</td>     <th>  No. Observations:  </th>  <td> 10000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  9998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th>    <td>1.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -798.23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Mon, 13 Feb 2017</td> <th>  Deviance:          </th> <td>  1596.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>14:46:35</td>     <th>  Pearson chi2:      </th> <td>7.15e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>        <td>11</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -10.6513</td> <td>    0.361</td> <td>  -29.491</td> <td> 0.000</td> <td>  -11.359    -9.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0055</td> <td>    0.000</td> <td>   24.952</td> <td> 0.000</td> <td>    0.005     0.006</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                10000\n",
       "Model:                            GLM   Df Residuals:                     9998\n",
       "Model Family:                Binomial   Df Model:                            1\n",
       "Link Function:                  logit   Scale:                             1.0\n",
       "Method:                          IRLS   Log-Likelihood:                -798.23\n",
       "Date:                Mon, 13 Feb 2017   Deviance:                       1596.5\n",
       "Time:                        14:46:35   Pearson chi2:                 7.15e+03\n",
       "No. Iterations:                    11                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -10.6513      0.361    -29.491      0.000       -11.359    -9.943\n",
       "balance        0.0055      0.000     24.952      0.000         0.005     0.006\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the `smf.glm` function to fit `default` to `balance` (hint: use the `binomial` family)\n",
    "logistic_model = smf.glm(formula='default ~ balance', data=df, family=sm.families.Binomial()).fit()\n",
    "logistic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a set of predicted probabilities using your model\n",
    "df['logistic_preds'] = logistic_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'logistic_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2a4c7412525d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Visualize the predicted probabilities across balances (bonus: add the observed values as well)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogistic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'logistic_model'"
     ]
    }
   ],
   "source": [
    "# Visualize the predicted probabilities across balances (bonus: add the observed values as well)\n",
    "plt.scatter(df.logistic_model, df.predictions)\n",
    "plt.plot(df.default, df.predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the simplicity of the `predict()` method makes it easy to overlook the interworkings of the model. You should note that the estimates are generated by plugging values of $X$ into this equation (with your estimated Beta values):\n",
    "\n",
    "\\begin{equation*}\n",
    "p(X) = \\frac { { e }^{ { { B }_{ 0 } } + { B }_{ 1 }X } }{ 1+{ e }^{ { { B }_{ 0 } } + { B }_{ 1 }X}} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Betas (15 minutes)\n",
    "\n",
    "As demonstrated above, the logistic formula is **linear in logit space**. This means that changes in your variables are associated with a **change in the log odds** of your dependent variable. There are a few implications of this:\n",
    "\n",
    "- **P-Values** retain their same interpretation (probability of observing the relationship _by chance_), and can be used to confirm/reject the null hypothesis\n",
    "- **Direction** of betas (+/-) is associated with a corresponding increase/decrease in the _log odds_ of the outcome. This can be interpreted as a positive / negative relationship. \n",
    "\n",
    "## Units\n",
    "**Units** of the betas are more difficult to interpret. Each unit increase in $X$ is _associated with_ an increase in the **log odds** of the dependent variable $Y$. As you can see in the curve above, increases in log odds are **multiplicative** in nature. This means that a unit increase in $X$ corresponds to a **different** change in $Y$ depending on your **current value of $X$** (see how the **slope** of the curve changes). However, if we **exponentiate the beta value**, we can also describe proportional increase in the **odds** (not just the increase in _log odds_). Consider the following formulas for calculating the log odds for ${X}_{i}$ and ${X}_{i - 1}$ (a unit increase in $X$):\n",
    "\n",
    "\\begin{equation*}\n",
    "LogOddsX_{i} = \\log \\left( { \\frac { p({X}_{i}) }{ 1-p({X}_{i}) } }\\right) ={ { B }_{ 0 } }+{ B }_{ 1 }{X}_{i}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "LogOddsX_{i-1} = \\log \\left( { \\frac { p({X}_{i-1}) }{ 1-p({X}_{i-1}) } }\\right) ={ { B }_{ 0 } }+{ B }_{ 1 }{X}_{i-1}\n",
    "\\end{equation*}\n",
    "\n",
    "Because of the **linear relationship** in logit space, we can then easily calculate the **difference** in log odds between ${X}_{i}$ and ${X}_{i + 1}$ as $B_{1}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\left( { \\frac { p({X}_{i}) }{ 1-p({X}_{i}) } }\\right) - \\log \\left( { \\frac { p({X}_{i-1}) }{ 1-p({X}_{i-1}) } }\\right) = {B}_{1}\n",
    "\\end{equation*}\n",
    "\n",
    "If we exponentiate each side of the equation, we can see that the **proportional increase in odds** betweeen $P(X_{i-1})$ and $P(X_{i})$ is simply ${e}^{{B}_{1}}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac { \\left( { \\frac { p({ X }_{ i }) }{ 1-p({ X }_{ i }) }  } \\right)  }{ \\left( { \\frac { p({ X }_{ i-1 }) }{ 1-p({ X }_{ i-1 }) }  } \\right)  } ={ e }^{ B_{ 1} }\n",
    "\\end{equation*}\n",
    "\n",
    "We can then interpret the **exponentiated beta value** as the observed **multiplicative unit increase in odds** in the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you should:\n",
    "- Get the `summary` of your logistic model to retrieve betas values from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve beta values from the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the beta values produced by the model, describe the relationship between `balance` and `default` (make sure to note the **p-value**, **direction**, and **units** of the coefficient):\n",
    "\n",
    "> Your insights here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More Interpretable Model (10 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportional increase of .6% in the model above is a bit small to have a good intuitive sense of. However, we can easily change our units of analysis to make this more interpretable. In the section below, you should do the following:\n",
    "\n",
    "- Convert balance to units of \\$100 and re-run the regression (hint, you'll need to create a new column or copy your dataframe)\n",
    "- Extract the beta values from your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert balance to units of $100 and re-run the regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the beta values from you model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the interpretation of your beta value from your regression?\n",
    "\n",
    "> Your insights here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics (10 minutes)\n",
    "\n",
    "In the section above, you've developed a strategy for interpreting the **betas** produced by your model. However, you also need to assess how well your model fits your data. In order to do this, we need to return to the purpose of our model. We want to **predict if people will default on their loans**. Before diving back into the code, we'll introduce a **confusion matrix** to represent the relationship between the data and our estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and Specificity\n",
    "The concepts of sensitivity and specificity express how well your model accurately predicts positive and negative classes. In order to compute these, it is easy to first build a **confusion matrix** to show the total number of cases that were accurately predicted (and those that were not).\n",
    "\n",
    "  <table>\n",
    "  <tr>\n",
    "    <td/>\n",
    "    <td/>\n",
    "    <td colspan=\"2\"><em>Predicted Class</em></td>\n",
    "    <td/>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td/>\n",
    "    <td/>\n",
    "    <td><strong>Negative (-)</strong></td>\n",
    "    <td><strong>Positive (+)</strong></td>\n",
    "    <td><strong>Total</strong></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\"><em>True Class</em></td>\n",
    "    <td><strong>Negative (-)</strong></td>\n",
    "    <td>True Negative (TN)</td>\n",
    "    <td>False Positives (FP)</td>\n",
    "    <td>Total non-cases (N)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Positive (+)</strong></td>\n",
    "    <td>False Negatives (FN)</td>\n",
    "    <td>True Positives (TP)</td>\n",
    "    <td>Total cases (P)</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix above, the rows represent the **data**, while the columns represent the **predictions**. For example, the top left cell (_True Negatives_) represents the number of instances in which someone did not default, and the model predicted that they did not default. Here are the definitions of each cell:\n",
    "\n",
    "- **True Positives**: The instances in which your model **predicts** a case (and the case **is true** in the data)\n",
    "- **False Positives**: The instances in which your model **predicts** a case (and the case **is not** true in the data)\n",
    "- **False Negatives**: The instances in which your model **does not** predict a case (and the case **is true** in the data)\n",
    "- **True Negatives**: The instances in which your model **does not** predict a case (and the case **is not** true in the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those values, we can compute the **sensitivity** and **specificity** of our model:\n",
    "\n",
    "\\begin{equation*}\n",
    "Sensitivity = \\frac { True Positives }{ True Positives+False Negatives } \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Specificity = \\frac { TrueNegatives }{ TrueNegatives+FalsePositives } \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two metrics lend important insights regarding the quality of our model:\n",
    "\n",
    "> _**Sensitivity**_, also referred to as the _true positive rate_, tells us, of all of the **cases in the data**, how many did we accurately predict? This indicates the model's **ability to detect cases**. In other words, how **sensitively** does the model pick up on cases?\n",
    "\n",
    "> _**Specificity**_, also referred to as the _true negative rate_, tells us, of all of the **non-cases in the data**, how many did we accurately predict? This indicates the model's ability to assign non-cases.\n",
    "\n",
    "These metrics are directly used to calculate **Type I and Type II error rate**, which are analagous to Type I and Type II errors in statistical tests (incorrectly reject a true null hypothesis, incorrectly accept a false null hypothesis). \n",
    "\n",
    "> **Type I Error** rate is the proportion of instances which are **incorrectly classified as positive cases** (relative to the total number of **negative cases**). It is calculated as $1-specificity$, or simply the false positives relative to the total non-cases in the data, $FP/N$.\n",
    "\n",
    "> **Type II Error** rate is the proportion of instances which are **incorrectly classified as negative cases** (relative to the total number of **positive cases**). It is calculated as $1-sensitivity$, or simply the false negatives relative to the total cases in the data, $FN/P$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metric Calculation (15 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll evaluate the fit of your model by calculating the following:\n",
    "\n",
    "- First, using a threshold of .5, use your model to predict a binary outcome (each case as 0 or 1)\n",
    "- What is the **accuracy** of your model (how often does the prediction match the data)?\n",
    "- What are the sensitivity, specificity, Type I error rate, and Type II error rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, using a threshold of .5, use your model to predict a binary outcome (each case as 0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the accuracy of your model (how often does the prediction match the data)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are the sensitivity, specificity, Type I error rate, and Type II error rates?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your interpretation of these values?\n",
    "\n",
    "> Your insights here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve (10 minutes)\n",
    "\n",
    "It is common to compare the _true positive rate_ (sensitivity) to the _false positive rate_ (1 - specificity) at each **threshold** for classification in an [ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). This [interactive visualization](http://www.navan.name/roc/) may help you better understand the relationship between thresholds and the ROC. In this section, you'll do the following:\n",
    "\n",
    "- Generate data for the ROC curve using the `metrics.roc_curve` function\n",
    "- Draw your ROC curve\n",
    "- Calculate the area under your ROC curve using the `metrics.roc_auc_curve` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate data for the ROC curve using the `metrics.roc_curve` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw your ROC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the area under your ROC curve using the metrics.roc_auc_curve function\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
